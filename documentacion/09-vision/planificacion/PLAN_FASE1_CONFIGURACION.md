# FASE 1: Configuraci√≥n y Preparaci√≥n

**Duraci√≥n estimada**: 1 semana
**Objetivo**: Preparar el entorno y la infraestructura base para procesamiento de frames del ESP32-CAM con MediaPipe.

---

## 1.1 Instalaci√≥n de Dependencias

### Paso 1.1.1: Agregar dependencias al `pubspec.yaml`

```yaml
dependencies:
  # Existentes (mantener)
  flutter:
    sdk: flutter
  flutter_bloc: ^8.1.3
  sensors_plus: ^3.0.2
  shelf: ^1.4.0  # Ya existe para HttpServerService

  # NUEVAS - Vision Processing
  google_mlkit_face_detection: ^0.10.0
  google_mlkit_pose_detection: ^0.11.0
  image: ^4.1.3  # Para conversi√≥n JPEG ‚Üí Image

dev_dependencies:
  flutter_test:
    sdk: flutter
  flutter_lints: ^2.0.0
```

**Justificaci√≥n de dependencias**:
- `google_mlkit_face_detection`: Para FaceMesh (468 landmarks, head pose, iris tracking)
- `google_mlkit_pose_detection`: Para detecci√≥n de manos (workaround, ya que ML Kit no tiene HandLandmarker nativo en Flutter)
- `image`: Para decodificar JPEG bytes del ESP32-CAM y convertir a formato compatible con InputImage

### Paso 1.1.2: Instalar dependencias

```bash
cd c:\Users\jdgut\Desktop\DriveGuard\android\app
flutter pub get
```

### Paso 1.1.3: Configurar permisos de Android (Opcional)

**NOTA**: Como usamos ESP32-CAM y NO la c√°mara del celular, NO se requieren permisos de c√°mara.

Archivo: `android/app/src/main/AndroidManifest.xml`

```xml
<manifest xmlns:android="http://schemas.android.com/apk/res/android">
    <application
        android:label="DriveGuard"
        android:name="${applicationName}"
        android:icon="@mipmap/ic_launcher">

        <!-- IMPORTANTE: NO agregar permisos de c√°mara -->
        <!-- Solo necesitamos permisos que ya existen: -->
        <!-- - INTERNET (para recibir frames del ESP32) -->
        <!-- - WAKE_LOCK (para mantener pantalla activa) -->

        <activity
            android:name=".MainActivity"
            android:exported="true"
            android:launchMode="singleTop"
            android:theme="@style/LaunchTheme"
            android:configChanges="orientation|keyboardHidden|keyboard|screenSize|smallestScreenSize|locale|layoutDirection|fontScale|screenLayout|density|uiMode"
            android:hardwareAccelerated="true"
            android:windowSoftInputMode="adjustResize"
            android:screenOrientation="portrait">
            <meta-data
              android:name="io.flutter.embedding.android.NormalTheme"
              android:resource="@style/NormalTheme"
              />
            <intent-filter>
                <action android:name="android.intent.action.MAIN"/>
                <category android:name="android.intent.category.LAUNCHER"/>
            </intent-filter>
        </activity>

        <meta-data
            android:name="flutterEmbedding"
            android:value="2" />
    </application>
</manifest>
```

---

## 1.2 Estructura de Archivos

### Paso 1.2.1: Crear estructura de directorios

```bash
# Desde la ra√≠z del proyecto
mkdir lib\core\vision
mkdir lib\core\vision\processors
mkdir lib\core\vision\detectors
mkdir lib\core\vision\models
mkdir lib\core\vision\utils
```

**Estructura resultante**:

```
lib/
‚îú‚îÄ‚îÄ core/
‚îÇ   ‚îú‚îÄ‚îÄ detection/           # Existente (IMU)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ detectors/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ processors/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ models/
‚îÇ   ‚îî‚îÄ‚îÄ vision/              # NUEVO (ESP32-CAM)
‚îÇ       ‚îú‚îÄ‚îÄ processors/      # FaceMeshProcessor, HandsProcessor
‚îÇ       ‚îú‚îÄ‚îÄ detectors/       # DistractionDetector, InattentionDetector, HandsOffDetector
‚îÇ       ‚îú‚îÄ‚îÄ models/          # VisionEvent, FaceData, HandData
‚îÇ       ‚îî‚îÄ‚îÄ utils/           # FrameConverter, ROICalibrator
```

---

## 1.3 Modelos de Datos Base

### Paso 1.3.1: Crear `VisionEvent` (lib/core/vision/models/vision_event.dart)

```dart
import 'package:equatable/equatable.dart';
import '../../detection/models/event_type.dart';
import '../../detection/models/event_severity.dart';

/// Evento de detecci√≥n basado en visi√≥n (ESP32-CAM)
class VisionEvent extends Equatable {
  final EventType type;
  final EventSeverity severity;
  final DateTime timestamp;
  final double confidence;
  final Map<String, dynamic> metadata;

  const VisionEvent({
    required this.type,
    required this.severity,
    required this.timestamp,
    required this.confidence,
    this.metadata = const {},
  });

  @override
  List<Object?> get props => [type, severity, timestamp, confidence, metadata];

  @override
  String toString() {
    return 'VisionEvent(type: $type, severity: $severity, confidence: ${confidence.toStringAsFixed(2)}, timestamp: $timestamp)';
  }
}
```

### Paso 1.3.2: Crear `FaceData` (lib/core/vision/models/face_data.dart)

```dart
import 'package:google_mlkit_face_detection/google_mlkit_face_detection.dart';

/// Datos procesados de detecci√≥n facial
class FaceData {
  final Face face;
  final double headYaw;        // Rotaci√≥n horizontal (-90¬∞ a 90¬∞)
  final double headPitch;      // Rotaci√≥n vertical (-90¬∞ a 90¬∞)
  final double headRoll;       // Inclinaci√≥n lateral (-180¬∞ a 180¬∞)
  final bool leftEyeOpen;
  final bool rightEyeOpen;
  final DateTime timestamp;

  FaceData({
    required this.face,
    required this.headYaw,
    required this.headPitch,
    required this.headRoll,
    required this.leftEyeOpen,
    required this.rightEyeOpen,
    required this.timestamp,
  });

  /// Determina si el conductor est√° mirando al frente
  bool get isLookingForward {
    // Tolerancia: ¬±20¬∞ en yaw (horizontal), ¬±15¬∞ en pitch (vertical)
    return headYaw.abs() < 20.0 && headPitch.abs() < 15.0;
  }

  /// Determina si el conductor tiene los ojos abiertos
  bool get hasEyesOpen {
    return leftEyeOpen && rightEyeOpen;
  }

  @override
  String toString() {
    return 'FaceData(yaw: ${headYaw.toStringAsFixed(1)}¬∞, '
        'pitch: ${headPitch.toStringAsFixed(1)}¬∞, '
        'roll: ${headRoll.toStringAsFixed(1)}¬∞, '
        'eyesOpen: $hasEyesOpen)';
  }
}
```

### Paso 1.3.3: Crear `HandData` (lib/core/vision/models/hand_data.dart)

```dart
import 'package:google_mlkit_pose_detection/google_mlkit_pose_detection.dart';

/// Datos procesados de detecci√≥n de manos (usando PoseDetector como workaround)
class HandData {
  final Pose pose;
  final bool leftHandInROI;   // Mano izquierda dentro de regi√≥n del volante
  final bool rightHandInROI;  // Mano derecha dentro de regi√≥n del volante
  final DateTime timestamp;

  HandData({
    required this.pose,
    required this.leftHandInROI,
    required this.rightHandInROI,
    required this.timestamp,
  });

  /// Cuenta cu√°ntas manos est√°n en el volante
  int get handsOnWheel {
    int count = 0;
    if (leftHandInROI) count++;
    if (rightHandInROI) count++;
    return count;
  }

  /// Determina si al menos una mano est√° en el volante
  bool get hasHandsOnWheel => handsOnWheel > 0;

  @override
  String toString() {
    return 'HandData(handsOnWheel: $handsOnWheel, '
        'leftInROI: $leftHandInROI, rightInROI: $rightHandInROI)';
  }
}
```

---

## 1.4 Extensi√≥n de Enumeraciones

### Paso 1.4.1: Extender `EventType` (lib/core/detection/models/event_type.dart)

**Agregar al enum existente**:

```dart
enum EventType {
  // Eventos IMU existentes
  harshBraking,
  aggressiveAcceleration,
  sharpTurn,
  weaving,
  roughRoad,
  speedBump,

  // NUEVOS - Eventos basados en visi√≥n (ESP32-CAM)
  distraction,        // Uso de tel√©fono m√≥vil
  inattention,        // Mirada fuera de la carretera
  handsOff,           // Ausencia de manos en el volante (h√≠brido)
}

extension EventTypeExtension on EventType {
  String get displayName {
    switch (this) {
      // Existentes
      case EventType.harshBraking:
        return 'Frenado Brusco';
      case EventType.aggressiveAcceleration:
        return 'Aceleraci√≥n Agresiva';
      case EventType.sharpTurn:
        return 'Giro Brusco';
      case EventType.weaving:
        return 'Zigzagueo';
      case EventType.roughRoad:
        return 'Camino Irregular';
      case EventType.speedBump:
        return 'Reductor de Velocidad';

      // NUEVOS - Visi√≥n
      case EventType.distraction:
        return 'Distracci√≥n (Tel√©fono)';
      case EventType.inattention:
        return 'Desatenci√≥n Visual';
      case EventType.handsOff:
        return 'Manos Fuera del Volante';
    }
  }

  String get description {
    switch (this) {
      // Existentes (mantener)
      case EventType.harshBraking:
        return 'El conductor fren√≥ bruscamente';
      case EventType.aggressiveAcceleration:
        return 'El conductor aceler√≥ agresivamente';
      case EventType.sharpTurn:
        return 'El conductor realiz√≥ un giro brusco';
      case EventType.weaving:
        return 'El veh√≠culo zigzague√≥ entre carriles';
      case EventType.roughRoad:
        return 'El veh√≠culo pas√≥ por un camino irregular';
      case EventType.speedBump:
        return 'El veh√≠culo pas√≥ por un reductor de velocidad';

      // NUEVOS - Visi√≥n
      case EventType.distraction:
        return 'El conductor est√° usando el tel√©fono m√≥vil';
      case EventType.inattention:
        return 'El conductor no est√° mirando la carretera';
      case EventType.handsOff:
        return 'El conductor no tiene las manos en el volante';
    }
  }

  /// NUEVO - Indica si el evento es basado en visi√≥n
  bool get isVisionBased {
    return this == EventType.distraction ||
           this == EventType.inattention ||
           this == EventType.handsOff;
  }

  /// NUEVO - Indica si el evento es h√≠brido (visi√≥n + IMU)
  bool get isHybrid {
    return this == EventType.handsOff; // Verifica visi√≥n + movimiento del veh√≠culo
  }
}
```

---

## 1.5 Utilidades de Conversi√≥n de Frames

### Paso 1.5.1: Crear `FrameConverter` (lib/core/vision/utils/frame_converter.dart)

**Prop√≥sito**: Convertir los frames JPEG del ESP32-CAM (recibidos como `Uint8List`) a `InputImage` compatible con MediaPipe.

```dart
import 'dart:typed_data';
import 'package:google_mlkit_face_detection/google_mlkit_face_detection.dart';
import 'package:image/image.dart' as img;

/// Convierte frames JPEG del ESP32-CAM a InputImage para MediaPipe
class FrameConverter {
  /// Convierte JPEG bytes a InputImage
  ///
  /// Proceso:
  /// 1. Decodificar JPEG ‚Üí Image (usando package 'image')
  /// 2. Convertir Image ‚Üí InputImage (formato compatible con ML Kit)
  static InputImage? fromJpegBytes(Uint8List jpegBytes) {
    try {
      // 1. Decodificar JPEG
      final image = img.decodeJpeg(jpegBytes);
      if (image == null) {
        print('[FrameConverter] ‚ùå Error al decodificar JPEG');
        return null;
      }

      // 2. Convertir a formato RGB (MediaPipe espera RGB)
      final rgbBytes = _imageToRgbBytes(image);

      // 3. Crear InputImageMetadata
      final metadata = InputImageMetadata(
        size: Size(image.width.toDouble(), image.height.toDouble()),
        rotation: InputImageRotation.rotation0deg, // ESP32-CAM est√° fijo
        format: InputImageFormat.yuv420,           // Formato compatible
        bytesPerRow: image.width * 3,              // RGB = 3 bytes por pixel
      );

      // 4. Crear InputImage
      final inputImage = InputImage.fromBytes(
        bytes: rgbBytes,
        metadata: metadata,
      );

      return inputImage;
    } catch (e) {
      print('[FrameConverter] ‚ùå Error en conversi√≥n: $e');
      return null;
    }
  }

  /// Convierte Image (package 'image') a bytes RGB
  static Uint8List _imageToRgbBytes(img.Image image) {
    final rgbBytes = <int>[];

    for (int y = 0; y < image.height; y++) {
      for (int x = 0; x < image.width; x++) {
        final pixel = image.getPixel(x, y);
        rgbBytes.add(pixel.r.toInt()); // Red
        rgbBytes.add(pixel.g.toInt()); // Green
        rgbBytes.add(pixel.b.toInt()); // Blue
      }
    }

    return Uint8List.fromList(rgbBytes);
  }

  /// Validar dimensiones del frame
  static bool validateFrameDimensions(Uint8List jpegBytes) {
    try {
      final image = img.decodeJpeg(jpegBytes);
      if (image == null) return false;

      // ESP32-CAM deber√≠a enviar 640x480 VGA
      final isValid = image.width == 640 && image.height == 480;

      if (!isValid) {
        print('[FrameConverter] ‚ö†Ô∏è Dimensiones inesperadas: '
            '${image.width}x${image.height} (esperado: 640x480)');
      }

      return isValid;
    } catch (e) {
      print('[FrameConverter] ‚ùå Error validando dimensiones: $e');
      return false;
    }
  }

  /// Obtener dimensiones del frame sin decodificar completamente
  static Size? getFrameSize(Uint8List jpegBytes) {
    try {
      final image = img.decodeJpeg(jpegBytes);
      if (image == null) return null;

      return Size(image.width.toDouble(), image.height.toDouble());
    } catch (e) {
      print('[FrameConverter] ‚ùå Error obteniendo tama√±o: $e');
      return null;
    }
  }
}
```

**Notas t√©cnicas**:
- **Por qu√© usar `package:image`**: ML Kit requiere `InputImage`, pero solo acepta formatos espec√≠ficos. El ESP32 env√≠a JPEG comprimido, as√≠ que necesitamos decodificarlo primero.
- **Formato RGB**: MediaPipe espera datos en formato RGB (3 bytes por pixel).
- **Rotaci√≥n fija**: Como el ESP32-CAM est√° montado en posici√≥n fija, siempre usamos `rotation0deg`.

---

## 1.6 Integraci√≥n con HttpServerService

### Paso 1.6.1: Verificar que HttpServerService est√° funcional

**Archivo existente**: `lib/data/datasources/local/http_server_service.dart`

**Verificar que tiene**:
```dart
final _frameController = StreamController<CameraFrame>.broadcast();
Stream<CameraFrame> get frameStream => _frameController.stream;

Future<Response> _handleImageUpload(Request request) async {
  // Recibe frames del ESP32-CAM
  final imageBytes = base64Decode(base64Image);
  final frame = CameraFrame.fromDecodedBytes(...);
  _frameController.add(frame);
}
```

**IMPORTANTE**: No modificar este archivo en Fase 1. Solo verificar que funciona.

### Paso 1.6.2: Crear servicio de suscripci√≥n a frames

**Archivo**: `lib/core/vision/utils/frame_subscriber.dart`

```dart
import 'dart:async';
import '../../../data/datasources/local/http_server_service.dart';
import '../../../data/models/camera_frame.dart';
import 'frame_converter.dart';
import 'package:google_mlkit_face_detection/google_mlkit_face_detection.dart';

/// Suscriptor a frames del ESP32-CAM con conversi√≥n autom√°tica
class FrameSubscriber {
  final HttpServerService _httpServerService;
  StreamSubscription<CameraFrame>? _frameSubscription;

  final _inputImageController = StreamController<InputImage>.broadcast();
  Stream<InputImage> get inputImageStream => _inputImageController.stream;

  FrameSubscriber(this._httpServerService);

  /// Inicia la suscripci√≥n a frames del ESP32-CAM
  void start() {
    _frameSubscription = _httpServerService.frameStream.listen((frame) {
      // Convertir JPEG ‚Üí InputImage
      final inputImage = FrameConverter.fromJpegBytes(frame.imageBytes);

      if (inputImage != null) {
        _inputImageController.add(inputImage);
      } else {
        print('[FrameSubscriber] ‚ö†Ô∏è Frame descartado (conversi√≥n fallida)');
      }
    });

    print('[FrameSubscriber] ‚úÖ Suscripci√≥n a frames iniciada');
  }

  /// Detiene la suscripci√≥n
  void stop() {
    _frameSubscription?.cancel();
    print('[FrameSubscriber] üõë Suscripci√≥n a frames detenida');
  }

  void dispose() {
    stop();
    _inputImageController.close();
  }
}
```

---

## 1.7 Pruebas de Verificaci√≥n

### Paso 1.7.1: Test de conversi√≥n de frames

**Archivo**: `test/core/vision/utils/frame_converter_test.dart`

```dart
import 'package:flutter_test/flutter_test.dart';
import 'package:driveguard/core/vision/utils/frame_converter.dart';
import 'dart:typed_data';

void main() {
  group('FrameConverter', () {
    test('debe rechazar bytes vac√≠os', () {
      final result = FrameConverter.fromJpegBytes(Uint8List(0));
      expect(result, isNull);
    });

    test('debe rechazar JPEG inv√°lido', () {
      final invalidJpeg = Uint8List.fromList([0xFF, 0xD8, 0x00]); // JPEG incompleto
      final result = FrameConverter.fromJpegBytes(invalidJpeg);
      expect(result, isNull);
    });

    // TODO: Agregar test con JPEG real del ESP32-CAM
  });
}
```

### Paso 1.7.2: Test de integraci√≥n con HttpServerService

**Verificaci√≥n manual**:

1. Asegurarse de que el ESP32-CAM est√© conectado y enviando frames
2. Abrir la app en modo debug
3. Ir a "Debug C√°mara ESP32"
4. Verificar logs:

```
‚úÖ Servidor HTTP iniciado en puerto 8080
üì° Esperando conexi√≥n del ESP32-CAM...
ü§ù Handshake recibido del ESP32-CAM
   IP ESP32: 192.168.43.100
üì∏ Frame recibido #1 (45678 bytes)
[FrameConverter] ‚úÖ Frame convertido: 640x480
[FrameSubscriber] ‚úÖ InputImage emitido
```

---

## 1.8 Checklist de Fase 1

Al finalizar esta fase, verificar:

- [ ] Dependencias instaladas (`flutter pub get` exitoso)
- [ ] Estructura de directorios creada (`lib/core/vision/`)
- [ ] Modelos creados (`VisionEvent`, `FaceData`, `HandData`)
- [ ] `EventType` extendido con eventos de visi√≥n
- [ ] `FrameConverter` implementado y probado
- [ ] `FrameSubscriber` implementado
- [ ] HttpServerService verificado funcional
- [ ] Tests unitarios escritos
- [ ] ESP32-CAM enviando frames exitosamente
- [ ] Frames siendo convertidos a `InputImage` sin errores

---

## 1.9 Problemas Comunes y Soluciones

### Problema: "MissingPluginException" al usar ML Kit

**Causa**: Plugins nativos no compilados.

**Soluci√≥n**:
```bash
cd android
./gradlew clean
cd ..
flutter clean
flutter pub get
flutter run
```

### Problema: "Failed to decode JPEG"

**Causa**: Frame corrupto o incompleto del ESP32-CAM.

**Soluci√≥n**:
1. Verificar calidad JPEG en ESP32 (deber√≠a ser 10-12)
2. Verificar que WiFi tenga buena se√±al
3. Agregar retry logic en FrameConverter

### Problema: Frames no llegan a FrameSubscriber

**Causa**: HttpServerService no est√° corriendo.

**Soluci√≥n**:
1. Asegurarse de abrir "Debug C√°mara ESP32" en la app
2. Verificar que el servidor inicie en puerto 8080
3. Verificar que ESP32 haga handshake exitoso

---

## Siguiente Fase

**FASE 2**: Implementaci√≥n de procesadores MediaPipe y detectores de eventos.

Ver: `PLAN_FASE2_IMPLEMENTACION.md`
