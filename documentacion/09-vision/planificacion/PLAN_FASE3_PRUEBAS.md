# FASE 3: Pruebas, Validaci√≥n y Mantenimiento

**Duraci√≥n estimada**: 1-2 semanas
**Objetivo**: Calibrar, validar y optimizar el sistema de detecci√≥n basado en visi√≥n.

---

## 3.1 Calibraci√≥n de ROI del Volante

### 3.1.1 ROICalibrator

**Archivo**: `lib/core/vision/utils/roi_calibrator.dart`

**Prop√≥sito**: Permitir al usuario definir manualmente la regi√≥n del volante en el frame del ESP32-CAM.

```dart
import 'dart:ui';

/// Calibrador de ROI (Region of Interest) del volante
class ROICalibrator {
  Rect? _steeringWheelROI;

  /// Configurar ROI manualmente (coordenadas relativas al frame 640x480)
  void setROI(Rect roi) {
    _steeringWheelROI = roi;
    print('[ROICalibrator] ‚úÖ ROI configurada: $roi');
  }

  /// Configurar ROI usando porcentajes (m√°s flexible)
  ///
  /// Ejemplo: Para un volante centrado que ocupa el 50% del ancho
  /// y est√° en la mitad inferior del frame:
  /// ```
  /// setROIFromPercentages(
  ///   leftPercent: 0.25,   // 25% desde la izquierda
  ///   topPercent: 0.50,    // 50% desde arriba
  ///   widthPercent: 0.50,  // 50% del ancho total
  ///   heightPercent: 0.30, // 30% del alto total
  /// );
  /// ```
  void setROIFromPercentages({
    required double leftPercent,
    required double topPercent,
    required double widthPercent,
    required double heightPercent,
  }) {
    const frameWidth = 640.0;
    const frameHeight = 480.0;

    final left = frameWidth * leftPercent;
    final top = frameHeight * topPercent;
    final width = frameWidth * widthPercent;
    final height = frameHeight * heightPercent;

    _steeringWheelROI = Rect.fromLTWH(left, top, width, height);
    print('[ROICalibrator] ‚úÖ ROI configurada (porcentajes): $_steeringWheelROI');
  }

  /// Configurar ROI predeterminada (volante centrado t√≠pico)
  void setDefaultROI() {
    // ROI t√≠pica para volante centrado en frame ESP32-CAM
    // Ocupa aproximadamente:
    // - 50% del ancho del frame
    // - 30% del alto del frame
    // - Centrado horizontalmente
    // - Posicionado en la mitad inferior
    setROIFromPercentages(
      leftPercent: 0.25,   // Centrado (25% margen izquierdo)
      topPercent: 0.50,    // Mitad inferior del frame
      widthPercent: 0.50,  // 50% del ancho
      heightPercent: 0.30, // 30% del alto
    );

    print('[ROICalibrator] ‚úÖ ROI predeterminada aplicada');
  }

  Rect? get roi => _steeringWheelROI;

  bool get isCalibrated => _steeringWheelROI != null;

  /// Guardar ROI en SharedPreferences (persistencia)
  Map<String, double> toJson() {
    if (_steeringWheelROI == null) {
      throw StateError('ROI no calibrada');
    }

    return {
      'left': _steeringWheelROI!.left,
      'top': _steeringWheelROI!.top,
      'width': _steeringWheelROI!.width,
      'height': _steeringWheelROI!.height,
    };
  }

  /// Cargar ROI desde SharedPreferences
  void fromJson(Map<String, double> json) {
    _steeringWheelROI = Rect.fromLTWH(
      json['left']!,
      json['top']!,
      json['width']!,
      json['height']!,
    );

    print('[ROICalibrator] ‚úÖ ROI cargada desde persistencia: $_steeringWheelROI');
  }

  void reset() {
    _steeringWheelROI = null;
    print('[ROICalibrator] üîÑ ROI reseteada');
  }
}
```

---

### 3.1.2 Widget de Calibraci√≥n Interactiva

**Archivo**: `lib/presentation/widgets/vision/roi_calibration_widget.dart`

**Prop√≥sito**: UI para que el usuario dibuje manualmente la ROI del volante sobre el frame en vivo.

```dart
import 'package:flutter/material.dart';
import 'dart:typed_data';
import '../../../core/vision/utils/roi_calibrator.dart';

/// Widget interactivo para calibrar ROI del volante
class ROICalibrationWidget extends StatefulWidget {
  final Uint8List frameBytes; // Frame actual del ESP32-CAM (JPEG)
  final ROICalibrator calibrator;
  final VoidCallback onCalibrationComplete;

  const ROICalibrationWidget({
    Key? key,
    required this.frameBytes,
    required this.calibrator,
    required this.onCalibrationComplete,
  }) : super(key: key);

  @override
  State<ROICalibrationWidget> createState() => _ROICalibrationWidgetState();
}

class _ROICalibrationWidgetState extends State<ROICalibrationWidget> {
  Offset? _startPoint;
  Offset? _endPoint;

  @override
  Widget build(BuildContext context) {
    return Scaffold(
      appBar: AppBar(
        title: const Text('Calibrar Volante'),
        actions: [
          IconButton(
            icon: const Icon(Icons.check),
            onPressed: _saveROI,
            tooltip: 'Guardar ROI',
          ),
          IconButton(
            icon: const Icon(Icons.refresh),
            onPressed: _useDefaultROI,
            tooltip: 'Usar ROI predeterminada',
          ),
        ],
      ),
      body: Column(
        children: [
          // Instrucciones
          Container(
            padding: const EdgeInsets.all(16),
            color: Colors.blue.shade100,
            child: const Text(
              'Dibuja un rect√°ngulo alrededor del volante.\n'
              'Mant√©n presionado y arrastra para definir la regi√≥n.',
              style: TextStyle(fontSize: 14),
              textAlign: TextAlign.center,
            ),
          ),

          // Frame con overlay de ROI
          Expanded(
            child: GestureDetector(
              onPanStart: (details) {
                setState(() {
                  _startPoint = details.localPosition;
                  _endPoint = null;
                });
              },
              onPanUpdate: (details) {
                setState(() {
                  _endPoint = details.localPosition;
                });
              },
              onPanEnd: (details) {
                // ROI dibujada, lista para guardar
              },
              child: CustomPaint(
                painter: ROIPainter(
                  frameBytes: widget.frameBytes,
                  startPoint: _startPoint,
                  endPoint: _endPoint,
                ),
                child: Container(),
              ),
            ),
          ),

          // Botones de acci√≥n
          Padding(
            padding: const EdgeInsets.all(16),
            child: Row(
              children: [
                Expanded(
                  child: ElevatedButton.icon(
                    onPressed: _saveROI,
                    icon: const Icon(Icons.save),
                    label: const Text('Guardar ROI'),
                  ),
                ),
                const SizedBox(width: 16),
                Expanded(
                  child: OutlinedButton.icon(
                    onPressed: _useDefaultROI,
                    icon: const Icon(Icons.auto_fix_high),
                    label: const Text('Usar Predeterminada'),
                  ),
                ),
              ],
            ),
          ),
        ],
      ),
    );
  }

  void _saveROI() {
    if (_startPoint == null || _endPoint == null) {
      ScaffoldMessenger.of(context).showSnackBar(
        const SnackBar(content: Text('Por favor, dibuja una regi√≥n primero')),
      );
      return;
    }

    // Convertir coordenadas de pantalla a coordenadas del frame (640x480)
    final roi = Rect.fromPoints(_startPoint!, _endPoint!);

    // Normalizar a coordenadas del frame
    // TODO: Escalar seg√∫n el tama√±o real del widget vs frame
    widget.calibrator.setROI(roi);

    ScaffoldMessenger.of(context).showSnackBar(
      const SnackBar(content: Text('‚úÖ ROI guardada exitosamente')),
    );

    widget.onCalibrationComplete();
  }

  void _useDefaultROI() {
    widget.calibrator.setDefaultROI();

    ScaffoldMessenger.of(context).showSnackBar(
      const SnackBar(content: Text('‚úÖ ROI predeterminada aplicada')),
    );

    widget.onCalibrationComplete();
  }
}

/// Painter para dibujar el frame y la ROI
class ROIPainter extends CustomPainter {
  final Uint8List frameBytes;
  final Offset? startPoint;
  final Offset? endPoint;

  ROIPainter({
    required this.frameBytes,
    this.startPoint,
    this.endPoint,
  });

  @override
  void paint(Canvas canvas, Size size) {
    // 1. Dibujar frame del ESP32-CAM
    // TODO: Decodificar frameBytes y dibujar imagen

    // 2. Dibujar ROI si existe
    if (startPoint != null && endPoint != null) {
      final rect = Rect.fromPoints(startPoint!, endPoint!);

      final paint = Paint()
        ..color = Colors.green.withOpacity(0.3)
        ..style = PaintingStyle.fill;

      final borderPaint = Paint()
        ..color = Colors.green
        ..style = PaintingStyle.stroke
        ..strokeWidth = 3;

      canvas.drawRect(rect, paint);
      canvas.drawRect(rect, borderPaint);

      // Dibujar texto de dimensiones
      final textPainter = TextPainter(
        text: TextSpan(
          text: '${rect.width.toInt()} x ${rect.height.toInt()}',
          style: const TextStyle(color: Colors.white, fontSize: 16),
        ),
        textDirection: TextDirection.ltr,
      );

      textPainter.layout();
      textPainter.paint(canvas, Offset(rect.left, rect.top - 25));
    }
  }

  @override
  bool shouldRepaint(covariant CustomPainter oldDelegate) => true;
}
```

---

## 3.2 Pruebas de Validaci√≥n

### 3.2.1 Test Unitarios - DistractionDetector

**Archivo**: `test/core/vision/detectors/distraction_detector_test.dart`

```dart
import 'package:flutter_test/flutter_test.dart';
import 'package:driveguard/core/vision/detectors/distraction_detector.dart';
import 'package:driveguard/core/vision/models/face_data.dart';
import 'package:driveguard/core/detection/models/event_type.dart';
import 'package:driveguard/core/detection/models/event_severity.dart';

void main() {
  group('DistractionDetector', () {
    late DistractionDetector detector;

    setUp(() {
      detector = DistractionDetector();
    });

    tearDown(() {
      detector.dispose();
    });

    test('no debe detectar distracci√≥n si headPitch > -25¬∞', () async {
      // Simular cara mirando al frente (pitch = -10¬∞)
      final faceData = _createFaceData(headPitch: -10.0);

      bool eventEmitted = false;
      detector.eventStream.listen((_) {
        eventEmitted = true;
      });

      // Procesar durante 3 segundos @ 5 FPS = 15 frames
      for (int i = 0; i < 15; i++) {
        detector.processFaceData(faceData);
        await Future.delayed(const Duration(milliseconds: 200));
      }

      expect(eventEmitted, isFalse);
    });

    test('debe detectar distracci√≥n si headPitch < -25¬∞ por 2+ segundos', () async {
      // Simular cara mirando hacia abajo (usando tel√©fono)
      final faceData = _createFaceData(headPitch: -30.0);

      VisionEvent? detectedEvent;
      detector.eventStream.listen((event) {
        detectedEvent = event;
      });

      // Procesar durante 2.5 segundos @ 5 FPS = 12 frames
      for (int i = 0; i < 12; i++) {
        detector.processFaceData(faceData);
        await Future.delayed(const Duration(milliseconds: 200));
      }

      expect(detectedEvent, isNotNull);
      expect(detectedEvent!.type, EventType.distraction);
      expect(detectedEvent!.severity, isIn([EventSeverity.low, EventSeverity.medium]));
    });

    test('debe aumentar severidad con mayor duraci√≥n', () async {
      final faceData = _createFaceData(headPitch: -35.0);

      VisionEvent? lastEvent;
      detector.eventStream.listen((event) {
        lastEvent = event;
      });

      // Procesar durante 7 segundos @ 5 FPS = 35 frames
      for (int i = 0; i < 35; i++) {
        detector.processFaceData(faceData);
        await Future.delayed(const Duration(milliseconds: 200));
      }

      // Esperar al menos un evento CRITICAL despu√©s de 6+ segundos
      expect(lastEvent, isNotNull);
      // Nota: Solo el primer evento se emite, pero severity deber√≠a ser alta
    });

    test('debe resetear detecci√≥n si la cara vuelve al frente', () async {
      final distractedFaceData = _createFaceData(headPitch: -30.0);
      final normalFaceData = _createFaceData(headPitch: -10.0);

      VisionEvent? detectedEvent;
      detector.eventStream.listen((event) {
        detectedEvent = event;
      });

      // Distracci√≥n por 1 segundo (5 frames)
      for (int i = 0; i < 5; i++) {
        detector.processFaceData(distractedFaceData);
        await Future.delayed(const Duration(milliseconds: 200));
      }

      // Volver al frente
      detector.processFaceData(normalFaceData);

      // Distracci√≥n nuevamente por 1.5 segundos (no deber√≠a detectar)
      for (int i = 0; i < 7; i++) {
        detector.processFaceData(distractedFaceData);
        await Future.delayed(const Duration(milliseconds: 200));
      }

      // No deber√≠a emitir evento porque se reseteo
      expect(detectedEvent, isNull);
    });
  });
}

// Helper para crear FaceData simulada
FaceData _createFaceData({
  double headYaw = 0.0,
  double headPitch = 0.0,
  double headRoll = 0.0,
}) {
  return FaceData(
    face: null as dynamic, // Mock (no se usa en detector)
    headYaw: headYaw,
    headPitch: headPitch,
    headRoll: headRoll,
    leftEyeOpen: true,
    rightEyeOpen: true,
    timestamp: DateTime.now(),
  );
}
```

---

### 3.2.2 Test Unitarios - HandsOffDetector (H√≠brido)

**Archivo**: `test/core/vision/detectors/hands_off_detector_test.dart`

```dart
import 'package:flutter_test/flutter_test.dart';
import 'package:driveguard/core/vision/detectors/hands_off_detector.dart';
import 'package:driveguard/core/vision/models/hand_data.dart';
import 'package:driveguard/domain/entities/sensor_data.dart';
import 'package:driveguard/core/detection/models/event_type.dart';

void main() {
  group('HandsOffDetector (H√≠brido)', () {
    late HandsOffDetector detector;

    setUp(() {
      detector = HandsOffDetector();
    });

    tearDown(() {
      detector.dispose();
    });

    test('NO debe detectar si el veh√≠culo est√° detenido (IMU inactivo)', () async {
      // Simular manos fuera del volante
      final handData = _createHandData(handsOnWheel: 0);

      // Simular veh√≠culo detenido (accel < 1.5, gyro < 20)
      final sensorData = _createSensorData(accelMagnitude: 0.5, gyroMagnitude: 5.0);

      VisionEvent? detectedEvent;
      detector.eventStream.listen((event) {
        detectedEvent = event;
      });

      // Procesar durante 5 segundos
      for (int i = 0; i < 25; i++) {
        detector.updateSensorData(sensorData);
        detector.processHandData(handData);
        await Future.delayed(const Duration(milliseconds: 200));
      }

      // NO debe emitir evento (veh√≠culo detenido)
      expect(detectedEvent, isNull);
    });

    test('S√ç debe detectar si el veh√≠culo est√° en movimiento (IMU activo)', () async {
      final handData = _createHandData(handsOnWheel: 0);

      // Simular veh√≠culo en movimiento (accel > 1.5)
      final sensorData = _createSensorData(accelMagnitude: 3.0, gyroMagnitude: 15.0);

      VisionEvent? detectedEvent;
      detector.eventStream.listen((event) {
        detectedEvent = event;
      });

      // Procesar durante 4 segundos @ 5 FPS
      for (int i = 0; i < 20; i++) {
        detector.updateSensorData(sensorData);
        detector.processHandData(handData);
        await Future.delayed(const Duration(milliseconds: 200));
      }

      // S√ç debe emitir evento (h√≠brido: sin manos + movimiento)
      expect(detectedEvent, isNotNull);
      expect(detectedEvent!.type, EventType.handsOff);
      expect(detectedEvent!.metadata['detectionMethod'], 'hybrid');
    });

    test('NO debe detectar si al menos 1 mano est√° en el volante', () async {
      // Simular 1 mano en el volante
      final handData = _createHandData(handsOnWheel: 1);

      final sensorData = _createSensorData(accelMagnitude: 3.0, gyroMagnitude: 15.0);

      VisionEvent? detectedEvent;
      detector.eventStream.listen((event) {
        detectedEvent = event;
      });

      for (int i = 0; i < 25; i++) {
        detector.updateSensorData(sensorData);
        detector.processHandData(handData);
        await Future.delayed(const Duration(milliseconds: 200));
      }

      // NO debe emitir evento (hay mano en el volante)
      expect(detectedEvent, isNull);
    });
  });
}

HandData _createHandData({required int handsOnWheel}) {
  return HandData(
    pose: null as dynamic, // Mock
    leftHandInROI: handsOnWheel >= 1,
    rightHandInROI: handsOnWheel >= 2,
    timestamp: DateTime.now(),
  );
}

SensorData _createSensorData({
  required double accelMagnitude,
  required double gyroMagnitude,
}) {
  return SensorData(
    timestamp: DateTime.now(),
    accelerationX: accelMagnitude / 1.732, // Distribuir magnitud en X,Y,Z
    accelerationY: accelMagnitude / 1.732,
    accelerationZ: accelMagnitude / 1.732,
    gyroscopeX: gyroMagnitude / 1.732,
    gyroscopeY: gyroMagnitude / 1.732,
    gyroscopeZ: gyroMagnitude / 1.732,
    isCalibrated: true,
  );
}
```

---

### 3.2.3 Test de Integraci√≥n - VisionProcessor

**Archivo**: `test/core/vision/processors/vision_processor_integration_test.dart`

```dart
import 'package:flutter_test/flutter_test.dart';
import 'package:driveguard/core/vision/processors/vision_processor.dart';
import 'package:driveguard/core/vision/models/vision_event.dart';
import 'dart:typed_data';

void main() {
  group('VisionProcessor Integration', () {
    late VisionProcessor processor;

    setUp(() {
      processor = VisionProcessor();
    });

    tearDown(() {
      processor.dispose();
    });

    test('debe emitir eventos consolidados de todos los detectores', () async {
      final events = <VisionEvent>[];
      processor.eventStream.listen((event) {
        events.add(event);
      });

      // TODO: Simular frames reales del ESP32-CAM
      // Por ahora, verificar que el stream est√° activo
      expect(processor.eventStream, isNotNull);
    });

    test('debe actualizar estad√≠sticas de procesamiento', () {
      final stats = processor.getStats();

      expect(stats, containsPair('faceMesh', isA<Map>()));
      expect(stats, containsPair('hands', isA<Map>()));
    });

    test('debe configurar ROI del volante', () {
      final roi = Rect.fromLTWH(100, 150, 300, 200);

      // No deber√≠a lanzar excepci√≥n
      expect(() => processor.setSteeringWheelROI(roi), returnsNormally);
    });
  });
}
```

---

## 3.3 Validaci√≥n en Condiciones Reales

### 3.3.1 Escenarios de Prueba

#### Escenario 1: Distracci√≥n (Uso de Tel√©fono)

**Configuraci√≥n**:
- ESP32-CAM montado en dashboard, apuntando al conductor
- Conductor sentado en posici√≥n normal de conducci√≥n

**Procedimiento**:
1. Iniciar monitoreo en la app
2. Conductor mira al frente durante 10 segundos (baseline)
3. Conductor mira hacia abajo simulando uso de tel√©fono durante 3 segundos
4. Volver a mirar al frente
5. Repetir 5 veces

**Validaci√≥n**:
- [ ] Evento de distracci√≥n detectado en 4/5 intentos (80% precisi√≥n)
- [ ] Tiempo de detecci√≥n: 2-3 segundos desde inicio de distracci√≥n
- [ ] Severidad incrementa con duraci√≥n
- [ ] No hay falsos positivos cuando mira al frente

**Logs esperados**:
```
[FaceMeshProcessor] üìä Procesados: 15 frames
[DistractionDetector] üö® Distracci√≥n detectada (duraci√≥n: 3s, severidad: MEDIUM)
[EventAggregator] ‚úÖ Evento agregado: DISTRACTION
```

---

#### Escenario 2: Inattention (Mirada Lateral)

**Procedimiento**:
1. Conductor mira al frente durante 10 segundos
2. Conductor gira la cabeza 45¬∞ a la izquierda durante 3 segundos
3. Volver al frente
4. Girar 45¬∞ a la derecha durante 3 segundos
5. Repetir 5 veces

**Validaci√≥n**:
- [ ] Evento de inattention detectado en 4/5 intentos por lado
- [ ] headYaw registrado correctamente (¬±45¬∞)
- [ ] No detecta cuando cabeza gira < 20¬∞

---

#### Escenario 3: Hands Off (H√≠brido)

**Procedimiento**:
1. Veh√≠culo detenido, conductor suelta el volante durante 5 segundos ‚Üí **NO debe alertar**
2. Veh√≠culo en movimiento (simular aceleraci√≥n), conductor suelta el volante durante 4 segundos ‚Üí **S√ç debe alertar**
3. Veh√≠culo en movimiento, conductor mantiene 1 mano en volante ‚Üí **NO debe alertar**

**Validaci√≥n**:
- [ ] NO alerta cuando veh√≠culo detenido (accel < 1.5)
- [ ] S√ç alerta cuando veh√≠culo en movimiento + manos fuera
- [ ] Metadata muestra `isMoving: true` cuando corresponde
- [ ] Confidence h√≠brida > 0.7

**Logs esperados**:
```
[HandsProcessor] üìä Manos detectadas: 0/2
[HandsOffDetector] üö® Manos fuera del volante detectado (duraci√≥n: 4s, severidad: MEDIUM)
  Metadata: {isMoving: true, accelMagnitude: 3.2, detectionMethod: 'hybrid'}
```

---

### 3.3.2 Matriz de Validaci√≥n

| Evento | Condici√≥n | Resultado Esperado | Precisi√≥n M√≠nima |
|--------|-----------|-------------------|------------------|
| **Distracci√≥n** | Mirando tel√©fono 3s | Detectado MEDIUM | 80% |
| **Distracci√≥n** | Mirando tel√©fono 7s | Detectado CRITICAL | 80% |
| **Distracci√≥n** | Mirando al frente | NO detectado | 95% |
| **Inattention** | Cabeza girada 45¬∞ por 3s | Detectado MEDIUM | 75% |
| **Inattention** | Cabeza girada 15¬∞ | NO detectado | 90% |
| **Hands Off** | Sin manos + detenido | NO detectado | 100% |
| **Hands Off** | Sin manos + movimiento 4s | Detectado MEDIUM | 80% |
| **Hands Off** | 1 mano + movimiento | NO detectado | 95% |

---

## 3.4 Optimizaci√≥n de Rendimiento

### 3.4.1 M√©tricas de Rendimiento

**Comando para monitorear FPS del procesamiento**:

```dart
// En VisionProcessor
class VisionProcessor {
  int _frameCount = 0;
  DateTime? _lastFpsReport;

  Future<void> processFrame(InputImage inputImage) async {
    _frameCount++;

    _lastFpsReport ??= DateTime.now();
    final elapsed = DateTime.now().difference(_lastFpsReport!);

    if (elapsed.inSeconds >= 5) {
      final fps = _frameCount / elapsed.inSeconds;
      print('[VisionProcessor] üìä FPS de procesamiento: ${fps.toStringAsFixed(1)}');

      _frameCount = 0;
      _lastFpsReport = DateTime.now();
    }

    // ... resto del c√≥digo
  }
}
```

**Objetivos**:
- FPS de procesamiento: **‚â• 4 FPS** (procesamos todos los frames del ESP32)
- Latencia frame ‚Üí evento: **‚â§ 500ms**
- Uso de CPU: **‚â§ 40%** promedio
- Uso de memoria: **‚â§ 150 MB** adicionales

---

### 3.4.2 Optimizaciones Comunes

#### Optimizaci√≥n 1: Skip frames si hay backlog

```dart
Future<void> processFrame(InputImage inputImage) async {
  if (_isProcessing) {
    _skippedFrames++;
    if (_skippedFrames % 10 == 0) {
      print('[VisionProcessor] ‚ö†Ô∏è Skipped $_skippedFrames frames (backlog)');
    }
    return; // Saltar frame
  }

  _isProcessing = true;
  // ... procesamiento
  _isProcessing = false;
}
```

#### Optimizaci√≥n 2: Reducir resoluci√≥n si necesario

```dart
// En ESP32-CAM main.cpp
config.frame_size = FRAMESIZE_VGA;  // 640x480 (actual)
// Si problemas de rendimiento:
// config.frame_size = FRAMESIZE_HVGA; // 480x320 (m√°s r√°pido)
```

#### Optimizaci√≥n 3: Procesar solo cada N frames

```dart
int _frameCounter = 0;

Future<void> processFrame(InputImage inputImage) async {
  _frameCounter++;

  // Procesar solo cada 2 frames (reduce a 2.5 FPS)
  if (_frameCounter % 2 != 0) {
    return;
  }

  // ... procesamiento
}
```

---

## 3.5 Debugging y Troubleshooting

### 3.5.1 P√°gina de Debug de Visi√≥n

**Archivo**: `lib/presentation/pages/esp32_vision_debug_page.dart`

```dart
import 'package:flutter/material.dart';
import '../../core/vision/processors/vision_processor.dart';
import '../../core/vision/utils/frame_subscriber.dart';
import '../../data/datasources/local/http_server_service.dart';

/// P√°gina de debug para sistema de visi√≥n
class ESP32VisionDebugPage extends StatefulWidget {
  const ESP32VisionDebugPage({Key? key}) : super(key: key);

  @override
  State<ESP32VisionDebugPage> createState() => _ESP32VisionDebugPageState();
}

class _ESP32VisionDebugPageState extends State<ESP32VisionDebugPage> {
  late VisionProcessor _visionProcessor;
  late FrameSubscriber _frameSubscriber;
  late HttpServerService _httpServerService;

  Map<String, dynamic> _stats = {};
  int _eventsDetected = 0;

  @override
  void initState() {
    super.initState();

    _httpServerService = HttpServerService();
    _visionProcessor = VisionProcessor();
    _frameSubscriber = FrameSubscriber(_httpServerService);

    _frameSubscriber.start();

    _frameSubscriber.inputImageStream.listen((inputImage) {
      _visionProcessor.processFrame(inputImage);
    });

    _visionProcessor.eventStream.listen((event) {
      setState(() {
        _eventsDetected++;
      });
    });

    // Actualizar estad√≠sticas cada segundo
    Future.periodic(const Duration(seconds: 1), (_) {
      setState(() {
        _stats = _visionProcessor.getStats();
      });
    });
  }

  @override
  Widget build(BuildContext context) {
    return Scaffold(
      appBar: AppBar(
        title: const Text('Debug Visi√≥n ESP32-CAM'),
      ),
      body: ListView(
        padding: const EdgeInsets.all(16),
        children: [
          // Estad√≠sticas FaceMesh
          _buildStatCard(
            'FaceMesh Processor',
            _stats['faceMesh'] as Map<String, dynamic>? ?? {},
            Icons.face,
          ),

          const SizedBox(height: 16),

          // Estad√≠sticas Hands
          _buildStatCard(
            'Hands Processor',
            _stats['hands'] as Map<String, dynamic>? ?? {},
            Icons.back_hand,
          ),

          const SizedBox(height: 16),

          // Eventos detectados
          Card(
            child: ListTile(
              leading: const Icon(Icons.warning_amber, color: Colors.orange),
              title: const Text('Eventos Detectados'),
              trailing: Text(
                '$_eventsDetected',
                style: const TextStyle(fontSize: 24, fontWeight: FontWeight.bold),
              ),
            ),
          ),

          const SizedBox(height: 16),

          // Bot√≥n para resetear detectores
          ElevatedButton.icon(
            onPressed: () {
              _visionProcessor.resetDetectors();
              setState(() {
                _eventsDetected = 0;
              });
            },
            icon: const Icon(Icons.refresh),
            label: const Text('Resetear Detectores'),
          ),
        ],
      ),
    );
  }

  Widget _buildStatCard(String title, Map<String, dynamic> stats, IconData icon) {
    return Card(
      child: Padding(
        padding: const EdgeInsets.all(16),
        child: Column(
          crossAxisAlignment: CrossAxisAlignment.start,
          children: [
            Row(
              children: [
                Icon(icon, color: Colors.blue),
                const SizedBox(width: 8),
                Text(
                  title,
                  style: const TextStyle(fontSize: 18, fontWeight: FontWeight.bold),
                ),
              ],
            ),
            const Divider(),
            ...stats.entries.map((entry) {
              return Padding(
                padding: const EdgeInsets.symmetric(vertical: 4),
                child: Row(
                  mainAxisAlignment: MainAxisAlignment.spaceBetween,
                  children: [
                    Text(entry.key),
                    Text(
                      '${entry.value}',
                      style: const TextStyle(fontWeight: FontWeight.bold),
                    ),
                  ],
                ),
              );
            }).toList(),
          ],
        ),
      ),
    );
  }

  @override
  void dispose() {
    _frameSubscriber.dispose();
    _visionProcessor.dispose();
    super.dispose();
  }
}
```

---

### 3.5.2 Gu√≠a de Troubleshooting

#### Problema: No se detectan eventos

**Diagn√≥stico**:
1. Verificar que frames llegan al VisionProcessor:
   ```
   [FrameSubscriber] ‚úÖ InputImage emitido
   [VisionProcessor] üìä FPS de procesamiento: 4.8
   ```

2. Verificar que MediaPipe procesa frames:
   ```
   [FaceMeshProcessor] üìä Procesados: 150 frames
   ```

3. Verificar umbrales de detecci√≥n:
   - ¬øheadPitch realmente < -25¬∞ cuando mira tel√©fono?
   - Agregar logs temporales en detectores

**Soluci√≥n**:
- Ajustar umbrales seg√∫n condiciones reales
- Verificar ROI del volante calibrada correctamente

---

#### Problema: Demasiados falsos positivos

**Causa com√∫n**: Umbrales muy permisivos o ROI muy grande.

**Soluci√≥n**:
1. Aumentar duraci√≥n m√≠nima de detecci√≥n:
   ```dart
   static const Duration _minDistractionDuration = Duration(seconds: 3); // Era 2
   ```

2. Ajustar umbrales de pose:
   ```dart
   static const double _downwardPitchThreshold = -30.0; // Era -25
   ```

3. Recalibrar ROI del volante (m√°s peque√±a)

---

#### Problema: Baja precisi√≥n de detecci√≥n de manos

**Causa**: PoseDetector es workaround, no tan preciso como HandLandmarker.

**Soluciones**:
1. **Opci√≥n 1**: Usar TFLite directo con modelo de manos
2. **Opci√≥n 2**: Expandir ROI del volante
3. **Opci√≥n 3**: Reducir umbral de duraci√≥n para compensar

---

## 3.6 Checklist Final de Fase 3

- [ ] ROI del volante calibrada y guardada en persistencia
- [ ] Tests unitarios pasan (DistractionDetector, InattentionDetector, HandsOffDetector)
- [ ] Tests de integraci√≥n completos
- [ ] Validaci√≥n en condiciones reales ‚â• 80% precisi√≥n
- [ ] Optimizaciones de rendimiento aplicadas
- [ ] FPS de procesamiento ‚â• 4 FPS
- [ ] P√°gina de debug funcional
- [ ] Documentaci√≥n de troubleshooting actualizada
- [ ] Falsos positivos < 10%
- [ ] Falsos negativos < 20%

---

## 3.7 Mantenimiento Continuo

### 3.7.1 Monitoreo Post-Lanzamiento

**M√©tricas a trackear**:
1. Tasa de detecci√≥n (eventos/hora de conducci√≥n)
2. Precisi√≥n por tipo de evento
3. FPS promedio de procesamiento
4. Crash rate relacionado con visi√≥n
5. Consumo de bater√≠a adicional

**Firebase Analytics (recomendado)**:
```dart
FirebaseAnalytics.instance.logEvent(
  name: 'vision_event_detected',
  parameters: {
    'event_type': event.type.name,
    'severity': event.severity.value,
    'confidence': event.confidence,
  },
);
```

---

### 3.7.2 Mejoras Futuras

#### Mejora 1: Hand Landmarker Nativo (cuando est√© disponible)
Reemplazar PoseDetector por HandLandmarker cuando ML Kit Flutter lo soporte.

#### Mejora 2: Detecci√≥n de Somnolencia (Drowsiness)
Agregar detector basado en:
- Frecuencia de parpadeo (blinks/min)
- Duraci√≥n de cierre de ojos
- Bostezos (mouth aspect ratio)

#### Mejora 3: Ajuste Din√°mico de Umbrales
Usar Machine Learning para ajustar umbrales seg√∫n:
- Patr√≥n de conducci√≥n del usuario
- Hora del d√≠a
- Condiciones de iluminaci√≥n

#### Mejora 4: Multi-Camera Support
Soportar m√∫ltiples ESP32-CAM:
- Una para el conductor
- Una para el camino (lane detection)

---

## 3.8 Documentaci√≥n Final

### Archivos de Documentaci√≥n a Crear

1. **USER_GUIDE_VISION.md**
   - C√≥mo calibrar ROI del volante
   - Posicionamiento √≥ptimo del ESP32-CAM
   - Interpretaci√≥n de alertas de visi√≥n

2. **API_REFERENCE_VISION.md**
   - Clases p√∫blicas (VisionProcessor, ROICalibrator)
   - Eventos y metadata
   - Extensibilidad para nuevos detectores

3. **PERFORMANCE_REPORT.md**
   - Benchmarks de FPS
   - Consumo de bater√≠a
   - Precisi√≥n por evento

---

## Conclusi√≥n de Fase 3

Al completar esta fase, el sistema de detecci√≥n basado en visi√≥n estar√°:

‚úÖ **Calibrado** con ROI del volante espec√≠fica del veh√≠culo
‚úÖ **Validado** con > 80% de precisi√≥n en condiciones reales
‚úÖ **Optimizado** para ‚â• 4 FPS de procesamiento
‚úÖ **Probado** con suite completa de tests unitarios e integraci√≥n
‚úÖ **Documentado** para usuarios y desarrolladores
‚úÖ **Listo para producci√≥n**

---

**Tiempo total estimado**: 4-6 semanas

**Pr√≥ximos pasos post-implementaci√≥n**: Ver secci√≥n 3.7.2 (Mejoras Futuras)
